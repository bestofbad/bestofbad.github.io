---
layout: my_archive
title: "TF2.x: 03. Logistic Classification (Regression)"
date: "2021-07-21"
categories: [DL, Tensorflow, tf-basic]
tags: [Deep Learning, Tensorflow, Basic Practice, Logistic Classification Regression]
sidebar:
  nav: "DL"
comments: true
---

이 포스트는 Github 접속 제약이 있을 경우를 위한 것이며, 아래와 동일 내용을 실행 결과와 함께 [Jupyter notebook](https://github.com/bestofbad/TF-Study/blob/main/tf2_03_Logistic_Classification_(Regression).ipynb)으로도 보실 수 있습니다.  
You can also see the following as [Jupyter notebook](https://github.com/bestofbad/TF-Study/blob/main/tf2_03_Logistic_Classification_(Regression).ipynb) along with execution result screens if you have no trouble connecting to the Github.
{: .notice--warning}

## 3-1. Train Data 확인

```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

x_train = [[1., 2.],
          [2., 3.],
          [3., 1.],
          [4., 3.],
          [5., 3.],
          [6., 2.]]
y_train = [[0.],
          [0.],
          [0.],
          [1.],
          [1.],
          [1.]]

x_test = [[5.,2.]]
y_test = [[1.]]


x1 = [x[0] for x in x_train]
x2 = [x[1] for x in x_train]

colors = [int(y[0] % 3) for y in y_train]
plt.scatter(x1,x2, c=colors , marker='^')
plt.scatter(x_test[0][0],x_test[0][1], c="red")

plt.xlabel("x1")
plt.ylabel("x2")
plt.show()

```

- tf.data.Dataset으로 학습시킬 data 담기 (Batch Size는 한번에 학습시킬 Size)
- features,labels는 실제 학습에 쓰일 Data (연산을 위해 Type를 맞추기)
```python
dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))       #.repeat()
```


## 3-2. 가중치 설정
```python
W = tf.Variable(tf.zeros([2,1]), name='weight')
b = tf.Variable(tf.zeros([1]), name='bias')
```

## 3-3. Sigmoid 함수를 가설로 선언
- tf.1.x 의 tf.div가 tf.divide로 변경됨
```python
def logistic_regression(features):
    hypothesis  = tf.divide(1., 1. + tf.exp(tf.matmul(features, W) + b))
    return hypothesis
```

## 3-4. 가설 검증할 Cost 함수 정의
- tf 1.x에서 tf.log가 tf.math.log로 변경됨
```python
def loss_fn(hypothesis, features, labels):
    cost = -tf.reduce_mean(labels * tf.math.log(logistic_regression(features)) + (1 - labels) * tf.math.log(1 - hypothesis))
    return cost

optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
```

## 3-5. 추론 값은 0.5 기준, 0과 1의 값을 리턴
- 가설을 통해 실재 값과 비교한 정확도를 측정합니다
```python
def accuracy_fn(hypothesis, labels):
    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))
    return accuracy
```

## 3-6. GradientTape를 통해 경사값 계산
```python
def grad(hypothesis, features, labels):
    with tf.GradientTape() as tape:
        loss_value = loss_fn(logistic_regression(features),features,labels)
    return tape.gradient(loss_value, [W,b])
```

## 3-7. 학습 실행
- 새로운 Data를 통한 검증 수행: [5,2] 로 테스트 수행 (그래프상 1이 나와야 정상)
```python
EPOCHS = 1001

for step in range(EPOCHS):
    for features, labels  in iter(dataset):
        grads = grad(logistic_regression(features), features, labels)
        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))
        if step % 100 == 0:
            print("Iter: {}, Loss: {:.4f}".format(step, loss_fn(logistic_regression(features),features,labels)))
test_acc = accuracy_fn(logistic_regression(x_test),y_test)
print("Testset Accuracy: {:.4f}".format(test_acc))
```

---
- [Reference](https://github.com/deeplearningzerotoall/TensorFlow/blob/master/tf_2.x/lab-05-1-logistic_regression-eager.ipynb)

{: .notice--info}